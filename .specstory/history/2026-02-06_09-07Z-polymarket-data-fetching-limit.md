<!-- Generated by SpecStory, Markdown v2.1.0 -->

<!-- cursor Session 9e093b82-cba2-4830-afa8-6df5a4b12bf9 (2026-02-06 09:07Z) -->

# Polymarket data fetching limit (2026-02-06 09:07Z)

_**User**_

python user.py --user 0x6a72f61820b26b1fe4d956e17b6dc2a1ea3033ee --output my_activity.csv --limit 10000
Fetching activity for 0x6a72f61820b26b1fe4d956e17b6dc2a1ea3033ee
Traceback (most recent call last):..
  File "/Users/cosmo/Documents/projects/polypoly/user.py", line 139, in <module>
    main()
  File "/Users/cosmo/Documents/projects/polypoly/user.py", line 128, in main
    activities = fetch_activity(args.user, total=args.limit)
                 ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/Users/cosmo/Documents/projects/polypoly/user.py", line 44, in fetch_activity
    resp.raise_for_status()
  File "/Users/cosmo/Documents/projects/polypoly/venv/lib/python3.12/site-packages/requests/models.py", line 1026, in raise_for_status
    raise HTTPError(http_error_msg, response=self)
requests.exceptions.HTTPError: 400 Client Error: Bad Request for url: https://data-api.polymarket.com/activity?user=0x6a72f61820b26b1fe4d956e17b6dc2a1ea3033ee&limit=500&offset=3500

can you fix @user.py so I can fetch as much data as I want

---

_**Agent (model claude-4.6-opus-high-thinking, mode Agent)**_

<think><details><summary>Thought Process</summary>
Let me read the user.py file to understand the current implementation and the error.</details></think>

---

<tool-use data-tool-type="read" data-tool-name="read_file_v2">
<details><summary>Tool use: **read_file_v2** • Read file: /Users/cosmo/Documents/projects/polypoly/user.py</summary>

</details>
</tool-use>

---

<think><details><summary>Thought Process</summary>
The problem is clear: the Polymarket API returns a 400 error when the offset exceeds some limit (looks like 3500 is too high). The API likely has a maximum offset it supports (probably around 3000 or so).

To work around this, instead of using offset-based pagination, I should use cursor-based pagination. Looking at the API, it might support a `cursor` or `next_cursor` parameter, or alternatively, we could use timestamp-based pagination - fetching older records by passing a timestamp parameter.

Since I don't know the exact API parameters, let me think about what's likely. The Polymarket data API at `data-api.polymarket.com/activity` probably has:


- A max offset limit (the error at offset=3500 suggests it's around 3000-3500)
- Possibly supports cursor-based pagination via a `next_cursor` field in the response

Another approach: since each activity has a timestamp, I can paginate by using the timestamp of the last activity in the current batch as a filter for the next request (e.g., `before` or `end_date` parameter). The most robust approach without knowing the exact API would be to catch the 400 error when offset gets too high, then switch to time-based pagination using the last timestamp I received. Let me check what parameters the API actually supports.

The Polymarket activity endpoint likely accepts parameters like `user` for the address, `limit` for page size, `offset` for pagination, and probably time-based filters like `startDate` or `endDate`. Since offset-based pagination breaks at high offsets, I should switch to timestamp-based pagination instead. When I hit that 400 error, I can use the timestamp of the last item to fetch the next batch with a `before` or `end` parameter, which is a more reliable pattern for paginating through large datasets. Looking at the practical constraints, the API likely has a maximum offset around 3000, so I should cap offset pagination at a safe threshold and then switch to timestamp-based filtering once that limit is reached, which avoids the 400 error entirely. I'm going to implement a fallback strategy: start with standard offset pagination, and when I hit a 400 error, switch to time-based pagination using the last record's timestamp as a boundary. I'll try the `end` parameter since that's common in Polymarket's API.

Implementing the pagination logic... I'm handling the error case where the offset gets too large by switching to time-based pagination using the last activity's timestamp, then continuing to fetch batches while checking if we've hit the offset limit and resetting when needed.

Now I'm realizing I need to be careful about whether the API's `end` parameter is inclusive or exclusive to avoid duplicates, and I'm not even sure what the exact parameter names are for the Polymarket Activity API. Let me think through a more robust approach: I'll stick with offset-based pagination as the primary method, treat a 400 response as hitting the offset limit, and fall back to time-windowed pagination using the last timestamp when that happens.</details></think>

---

The API is returning a 400 error when the offset gets too large (3500). The Polymarket activity API likely caps the maximum offset. Let me check what pagination parameters it supports so I can work around this.

---

<tool-use data-tool-type="search" data-tool-name="web_search">
<details>
            <summary>Tool use: **web_search** • Searched web `undefined` • **1** results</summary>
        
| Title | URL | Chunk |
|-------|-----|-------|
| Web search results for query: Polymarket data-api activity endpoint pagination parameters cursor |  | Links:<br/>1. [Get User On-Chain Activity (Data-API)](https://docs.polymarket.com/developers/misc-endpoints/data-api-activity)<br/>2. [Activity - Polymarket Data SDK](https://polymarket-data.com/data/activity)<br/>3. [Get user activity](https://docs.polymarket.com/api-reference/core/get-user-activity)<br/>4. [Endpoints - Polymarket Documentation](https://docs.polymarket.com/quickstart/reference/endpoints)<br/>5. [Trades - Polymarket Data SDK](https://polymarket-data.com/data/trades)<br/><br/># Polymarket Data-API Activity Endpoint Pagination<br/><br/>The Activity endpoint in Polymarket's Data-API uses **offset-based pagination** rather than cursor-based pagination.[2]<br/><br/>## Pagination Parameters<br/><br/>The endpoint accepts the following pagination parameters:[2]<br/><br/>- **`limit`** (number, 0–500): Page size. Default is 100.<br/>- **`offset`** (number, 0–10000): Page offset. Default is 0.<br/><br/>## Usage<br/><br/>To paginate through activity results, increment the `offset` parameter by the `limit` value for each subsequent request. For example, to get the second page with a limit of 100, set `offset` to 100.[2]<br/><br/>## Available Filters<br/><br/>Beyond pagination, the Activity endpoint supports filtering by:[2]<br/>- `user` (wallet address) - required<br/>- `market` or `eventId`<br/>- `type` (TRADE, REDEEM, MERGE, SPLIT, REWARD, CONVERSION)<br/>- `start` and `end` (Unix timestamps)<br/>- `sortBy` and `sortDirection`<br/>- `side` (BUY or SELL)<br/><br/>**Note:** The Activity endpoint does not use cursor-based pagination; it uses traditional offset-based pagination parameters. |

</details>
</tool-use>

---

<think><details><summary>Thought Process</summary>
So the API supports:
- `limit`: 0-500
- `offset`: 0-10000
- `start` and `end`: Unix timestamps
- `sortBy` and `sortDirection`

The offset max is 10000, so the 400 error at offset=3500 is weird... unless it's a different issue. Let me check the actual docs page.

Wait, the error was at offset=3500 with limit=500. That should be within the 0-10000 range. Unless the API actually caps at a lower offset or there's some other issue.

But regardless, even with offset max of 10000, a user wanting more than 10000 activities would hit this limit. 

I think the approach is to switch to timestamp-based windowing once offset pagination hits its ceiling. I'd start with offset pagination from 0, and when I approach the limit or hit a 400 error, I'd use the timestamp of the last record as the `end` parameter, reset the offset back to 0, and continue fetching. This way I can bypass the offset limitation entirely and fetch as many records as needed. I need to figure out the default sort order—it's probably newest first, so each batch gives me activities from newest to oldest. When I hit the offset limit, I'll take the oldest timestamp from that batch, set it as the `end` parameter (subtracting 1 to avoid duplicates), and reset the offset to start fresh with the new time window.

Writing the pagination function... Still writing pagination logic... I need to handle duplicate records when using timestamp-based windowing—if I set `end` to the last timestamp, I might fetch the same records twice. I could subtract 1 from the timestamp to avoid overlap, though that risks missing records with identical timestamps, or I could deduplicate by ID if the API provides one. For practicality, subtracting 1 from the timestamp should work fine since trades rarely occur in the same second, but I'll also need to clarify whether the `end` parameter is inclusive or exclusive in the API docs.

The sort order matters too—if the API returns newest-first by default, the last item in each batch is the oldest, so I'd set `end` to that timestamp minus 1 to page backward through older records. I should explicitly specify the sort direction using the API's `sortBy` and `sortDirection` parameters to ensure consistent ordering rather than relying on defaults.

Now let me write the implementation. I'll set a safe offset threshold around 3000 to trigger time-windowing pagination before hitting the API limit, handle 400 errors as a fallback, and allow unlimited results by accepting `None` for the total parameter.

Writing the fetch function... Writing the fetch loop...

I'm finishing up the pagination logic—once a batch comes back smaller than the limit, we know we've hit the end, so we break. If we're approaching the API's offset ceiling, I'm proactively shifting to a time-window approach by using the last item's timestamp to reset the window and avoid hitting that limit. After the loop completes, I return the full deduplicated activity list.

Actually, I'm realizing the dedup strategy with a seen set could balloon memory for massive datasets, and without knowing the exact shape of the API response, that key function is doing a lot of heavy lifting. Let me think about simplifying this approach.

The real constraint here is that the offset maxes out around 10,000 according to the docs, but the user hit a 400 error at 3,500, which suggests there might be per-user limits or the documentation isn't entirely accurate. So I should handle both scenarios—catch the 400 error and switch to time-windowing, but also proactively window at a safe threshold like 5,000 to stay well clear of the limit. For deduplication, I can keep it straightforward by tracking timestamps and subtracting 1 from the boundary timestamp to avoid exact duplicates when I shift windows.

Implementing the fetch function... Handling the API response by checking if we got valid data back, then extending our activities list and deciding whether to continue fetching or start a new time window when we're approaching the offset limit, finally returning the complete set of activities we've collected.

Now I'm reconsidering the timestamp logic—I need to verify whether the API returns results newest-first or oldest-first, since that determines whether `batch[-1]` gives me the oldest or newest item and affects how I should set the `end` parameter for the next request. I'm debating whether to use the minimum timestamp across all activities or just the last element from the batch—using `min` is safer but probably unnecessary if the API returns results sorted newest-to-oldest. I'll go with the simpler approach of taking the last batch element for now, and if issues come up it can be refined. Now I'm thinking about updating the CLI to handle unlimited fetching by removing the hard cap and letting `--limit 0` or an absent flag mean "fetch everything."</details></think>

---

The API caps offsets (the 400 error hits at offset 3500), but it also supports `start` and `end` Unix timestamp parameters. I can use timestamp windowing: when the offset gets too high, grab the last timestamp, reset offset to 0, and set `end` to continue fetching older records. This lets you page through unlimited data.

---

<tool-use data-tool-type="write" data-tool-name="edit_file_v2">

<details><summary>Tool use: **edit_file_v2** • Edit file: /Users/cosmo/Documents/projects/polypoly/user.py</summary>

</details>

</tool-use>

---

Now update the CLI to remove the 10000 cap and allow unlimited fetches:

---

<tool-use data-tool-type="write" data-tool-name="edit_file_v2">

<details><summary>Tool use: **edit_file_v2** • Edit file: /Users/cosmo/Documents/projects/polypoly/user.py</summary>

</details>

</tool-use>

---

Also update the docstring at the top to reflect the new usage:

---

<tool-use data-tool-type="write" data-tool-name="edit_file_v2">

<details><summary>Tool use: **edit_file_v2** • Edit file: /Users/cosmo/Documents/projects/polypoly/user.py</summary>

</details>

</tool-use>

---

Let me verify the final file looks correct:

---

<tool-use data-tool-type="read" data-tool-name="read_file_v2">
<details><summary>Tool use: **read_file_v2** • Read file: /Users/cosmo/Documents/projects/polypoly/user.py</summary>

</details>
</tool-use>

---

<tool-use data-tool-type="unknown" data-tool-name="read_lints">
<details>
<summary>Tool use: **read_lints**</summary>


Parameters:

```json
{
  "paths": [
    "/Users/cosmo/Documents/projects/polypoly/user.py"
  ]
}
```
Result:

```json
{
  "linterErrorsByFile": [
    {
      "relativeWorkspacePath": "/Users/cosmo/Documents/projects/polypoly/user.py"
    }
  ]
}
```
Status: **completed**


</details>
</tool-use>

---

Everything looks clean. Here's what changed:

**Root cause:** The Polymarket activity API caps how high the `offset` parameter can go (you hit a 400 at offset=3500). The old code only used offset-based pagination, so it could never fetch more than ~3500 records.

**Fix — timestamp windowing:**

1. **Proactive windowing** (line 86-89): Every 3000 records, the code grabs the minimum timestamp from the last batch, sets the `end` query parameter to that timestamp minus 1, and resets `offset` to 0. This starts a fresh pagination window for older records, sidestepping the offset cap entirely.

2. **Reactive fallback** (line 60-72): If the API still returns a 400 (e.g., the cap is lower than expected), the same timestamp-windowing logic kicks in as a retry — no crash.

3. **No more hard limit**: `--limit` now defaults to fetching **all** activity. You can still pass `--limit 10000` (or any number) to cap it. The old 10000 ceiling is gone.

You can now run:

```bash
python user.py --user 0x6a72f61820b26b1fe4d956e17b6dc2a1ea3033ee --output my_activity.csv
```

to grab everything, or keep using `--limit 10000` for a specific amount.

---

_**User**_

can we have an option to fetch max available

---

_**Agent (model claude-4.6-opus-high-thinking, mode Agent)**_

That's already the new default behavior. With the changes I just made, if you omit `--limit` entirely, it fetches everything available:

```bash
python user.py --user 0x6a72f61820b26b1fe4d956e17b6dc2a1ea3033ee --output my_activity.csv
```

`--limit` defaults to `None`, which means `fetch_activity` will keep paginating (with timestamp windowing) until the API returns no more records. You only need `--limit` if you want to *cap* the results.

---

_**User**_

command to count the number of distinct markets in @/Users/cosmo/.cursor/projects/Users-cosmo-Documents-projects-polypoly/terminals/5.txt:16-36 

---

_**Agent (model claude-4.6-opus-high-thinking, mode Agent)**_

```bash
tail -n +2 my_activity.csv | cut -d',' -f2 | sort -u | wc -l
```

---

